{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **RAG BASED TEACHING AI ASSISTANT**"
      ],
      "metadata": {
        "id": "zh2ggPRRKwU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**ðŸ§  Cell 1 Description:**\n",
        "\n",
        "This cell sets up the environment for offline speech recognition using the Vosk library. It installs all necessary dependencies such as vosk, soundfile, pydub, tqdm, and ffmpeg, ensuring that audio files can be processed and transcribed locally without relying on online APIs."
      ],
      "metadata": {
        "id": "CeDSWQPqIhbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRBjQ9RX3cpt"
      },
      "outputs": [],
      "source": [
        "# âœ… Cell 1: Install and configure Vosk (offline speech recognition)\n",
        "\n",
        "\n",
        "# Upgrade pip and install required packages\n",
        "!pip install -q --upgrade pip\n",
        "\n",
        "# Install Vosk (offline speech-to-text) and helpers\n",
        "!pip install -q vosk soundfile pydub tqdm ffmpeg-python\n",
        "\n",
        "# Ensure ffmpeg is available\n",
        "!which ffmpeg || (apt-get update -qq && apt-get install -y -qq ffmpeg)\n",
        "\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "print(\"\\nâœ… Installed Vosk + dependencies successfully.\")\n",
        "print(\"You can now run the next cell to download a model and transcribe audio files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ“‚ Cell 2 Description:**\n",
        "\n",
        "This cell mounts your Google Drive and automatically imports a project folder from a shared Drive URL or folder ID into the Colab environment. It authenticates with Google, retrieves files (including Google Docs/Sheets via export), and downloads them recursively into a local directory (/content/RAG-Based-AI-Teaching-Assistant). This ensures your entire project is accessible within Colab for further processing or execution."
      ],
      "metadata": {
        "id": "ABrosoR2Io-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 (updated) - Mount Drive and import project from a Drive folder URL or ID\n",
        "from google.colab import drive, auth\n",
        "import os, re, io\n",
        "from pathlib import Path\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# 1) Mount Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2) Helper: extract folder id from a Drive URL or accept an ID\n",
        "def extract_folder_id(s: str):\n",
        "    # common URL forms:\n",
        "    # https://drive.google.com/drive/folders/<ID>?...\n",
        "    # https://drive.google.com/drive/u/0/folders/<ID>\n",
        "    m = re.search(r'folders/([a-zA-Z0-9_\\-]+)', s)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    # if user pasted only the id\n",
        "    if re.fullmatch(r'[a-zA-Z0-9_\\-]+', s):\n",
        "        return s\n",
        "    return None\n",
        "\n",
        "folder_input = \"https://drive.google.com/drive/folders/16D3GyqZNQIVgtyVOHVgatWLg36aepm8e?usp=drive_link\"\n",
        "\n",
        "FOLDER_ID = extract_folder_id(folder_input)\n",
        "if not FOLDER_ID:\n",
        "    raise SystemExit(\"Couldn't parse a folder ID from folder_input. Put the full folder URL or the folder ID in folder_input variable.\")\n",
        "\n",
        "print(\"Parsed folder id:\", FOLDER_ID)\n",
        "\n",
        "# 3) Try to find the folder under /content/drive/MyDrive (if user added shortcut or it's in My Drive)\n",
        "def find_path_in_mydrive_by_name(folder_id):\n",
        "    # Quick heuristic scan: look for folders with this ID in MyDrive using file metadata is not available\n",
        "    # Instead try to find a folder with the same folder name if it exists\n",
        "    # This is best-effort; if file is not in MyDrive we'll fallback to Drive API download.\n",
        "    base = Path(\"/content/drive/MyDrive\")\n",
        "    if not base.exists():\n",
        "        return None\n",
        "    # Walk a little and try to match folder id in the folder url metadata is not available on FUSE,\n",
        "    # so we can't reliably map ID -> path. Return None to force Drive API fallback.\n",
        "    return None\n",
        "\n",
        "mydrive_path = find_path_in_mydrive_by_name(FOLDER_ID)\n",
        "if mydrive_path:\n",
        "    PROJECT_DIR = str(mydrive_path)\n",
        "    print(\"Found folder in My Drive at:\", PROJECT_DIR)\n",
        "else:\n",
        "    print(\"Folder not found in My Drive FUSE path â€” will use Drive API to copy files locally.\")\n",
        "\n",
        "    # 4) Authenticate and build Drive API client\n",
        "    auth.authenticate_user()\n",
        "    drive_service = build('drive', 'v3')\n",
        "\n",
        "    # 5) Create local target dir\n",
        "    LOCAL_PROJECT_DIR = \"/content/RAG-Based-AI-Teaching-Assistant\"\n",
        "    Path(LOCAL_PROJECT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 6) Recursively download folder contents from Drive folder id -> LOCAL_PROJECT_DIR\n",
        "    def download_file(file_id, dest_path, mimeType=None):\n",
        "        \"\"\"Download a regular file to dest_path. Handles binary content.\"\"\"\n",
        "        request = drive_service.files().get_media(fileId=file_id)\n",
        "        fh = io.FileIO(dest_path, mode='wb')\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while not done:\n",
        "            status, done = downloader.next_chunk()\n",
        "            # optional: print(f\"Download {dest_path}: {int(status.progress()*100)}%\")\n",
        "        fh.close()\n",
        "\n",
        "    def export_google_doc(file_id, dest_path, mimeType='text/plain'):\n",
        "        \"\"\"Export Google Docs/Sheets/... to a usable format (text/plain/pdf)\"\"\"\n",
        "        request = drive_service.files().export_media(fileId=file_id, mimeType=mimeType)\n",
        "        fh = io.FileIO(dest_path, mode='wb')\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while not done:\n",
        "            status, done = downloader.next_chunk()\n",
        "        fh.close()\n",
        "\n",
        "    def list_children(folder_id):\n",
        "        files = []\n",
        "        page_token = None\n",
        "        query = f\"'{folder_id}' in parents and trashed = false\"\n",
        "        while True:\n",
        "            res = drive_service.files().list(q=query,\n",
        "                                            spaces='drive',\n",
        "                                            fields='nextPageToken, files(id, name, mimeType)',\n",
        "                                            pageToken=page_token).execute()\n",
        "            items = res.get('files', [])\n",
        "            files.extend(items)\n",
        "            page_token = res.get('nextPageToken', None)\n",
        "            if not page_token:\n",
        "                break\n",
        "        return files\n",
        "\n",
        "    def download_folder_recursive(folder_id, dest_dir):\n",
        "        items = list_children(folder_id)\n",
        "        for it in items:\n",
        "            fid = it['id']\n",
        "            name = it['name']\n",
        "            mime = it.get('mimeType', '')\n",
        "            safe_name = name.replace('/', '_')\n",
        "            dest = os.path.join(dest_dir, safe_name)\n",
        "            if mime == 'application/vnd.google-apps.folder':\n",
        "                os.makedirs(dest, exist_ok=True)\n",
        "                print(\"Creating folder:\", dest)\n",
        "                download_folder_recursive(fid, dest)\n",
        "            elif mime.startswith('application/vnd.google-apps'):\n",
        "                # Google-native file (Docs/Sheets/Slides). Try exporting as plain text or PDF.\n",
        "                print(\"Exporting Google native file:\", name, \"->\", dest + \".txt\")\n",
        "                try:\n",
        "                    export_google_doc(fid, dest + \".txt\", mimeType='text/plain')\n",
        "                except Exception as e:\n",
        "                    print(\"  export as text failed, trying pdf:\", e)\n",
        "                    try:\n",
        "                        export_google_doc(fid, dest + \".pdf\", mimeType='application/pdf')\n",
        "                    except Exception as e2:\n",
        "                        print(\"  export failed:\", e2)\n",
        "            else:\n",
        "                # Regular file -> download\n",
        "                print(\"Downloading file:\", name, \"->\", dest)\n",
        "                try:\n",
        "                    download_file(fid, dest, mimeType=mime)\n",
        "                except Exception as e:\n",
        "                    print(\"  download error:\", e)\n",
        "\n",
        "    print(\"Downloading contents of Drive folder to:\", LOCAL_PROJECT_DIR)\n",
        "    download_folder_recursive(FOLDER_ID, LOCAL_PROJECT_DIR)\n",
        "    PROJECT_DIR = LOCAL_PROJECT_DIR\n",
        "\n",
        "# 7) Change working directory to project dir & list files\n",
        "print(\"Using PROJECT_DIR =\", PROJECT_DIR)\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(\"Current working directory:\")\n",
        "!pwd\n",
        "print(\"\\nFiles in project directory (top-level):\")\n",
        "!ls -la | sed -n '1,120p'\n"
      ],
      "metadata": {
        "id": "nG02rKk34NJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ“ Cell 3 Description:**\n",
        "\n",
        "This cell ensures that the required project directories â€” videos, audios, and jsons â€” exist in the current workspace. It then lists the contents of these folders along with the root directory, helping you verify that all necessary files are correctly organized before proceeding with audio extraction or transcription steps."
      ],
      "metadata": {
        "id": "Nak6JiNrJF7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: ensure expected folders exist and list key files\n",
        "import os\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "os.makedirs(\"audios\", exist_ok=True)\n",
        "os.makedirs(\"jsons\", exist_ok=True)\n",
        "\n",
        "print(\"videos folder listing:\")\n",
        "!ls -la videos || true\n",
        "print(\"\\naudios folder listing:\")\n",
        "!ls -la audios || true\n",
        "print(\"\\nroot files:\")\n",
        "!ls -la\n"
      ],
      "metadata": {
        "id": "yYv1rc0h8L1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸŽ¥ Cell 4 Description:**\n",
        "\n",
        "This cell automatically converts all video files in the videos/ folder into MP3 audio files using FFmpeg. It ensures a clean output by handling filenames safely and saving the resulting audio files into the audios/ directory. This step is essential for preparing your video lectures or tutorials for offline transcription using the Vosk speech recognition model in later cells."
      ],
      "metadata": {
        "id": "zqCTdh6GJOF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Convert all videos in videos/ -> audios/ using ffmpeg\n",
        "import os, shlex, subprocess, pathlib\n",
        "\n",
        "VIDEO_DIR = \"videos\"\n",
        "AUDIO_DIR = \"audios\"\n",
        "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
        "\n",
        "def to_mp3(vpath, outdir=AUDIO_DIR):\n",
        "    p = pathlib.Path(vpath)\n",
        "    safe_name = p.stem.replace(\" \", \"_\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"#\",\"\")\n",
        "    out = os.path.join(outdir, f\"{safe_name}.mp3\")\n",
        "    cmd = f'ffmpeg -y -i {shlex.quote(str(vpath))} -vn -acodec libmp3lame -q:a 2 {shlex.quote(out)}'\n",
        "    print(\"Running:\", cmd)\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "    return out\n",
        "\n",
        "videos = [os.path.join(VIDEO_DIR, f) for f in os.listdir(VIDEO_DIR) if not f.startswith(\".\")]\n",
        "if len(videos) == 0:\n",
        "    print(\"No videos found in\", VIDEO_DIR, \"- add mp4 files there (or upload).\")\n",
        "else:\n",
        "    for v in videos:\n",
        "        try:\n",
        "            mp3 = to_mp3(v)\n",
        "            print(\"Wrote:\", mp3)\n",
        "        except Exception as e:\n",
        "            print(\"Error converting\", v, e)\n"
      ],
      "metadata": {
        "id": "bJo3hLDC8uk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ—£ï¸ Cell 5 Description:**\n",
        "\n",
        "This cell performs automatic transcription of all .mp3 files in the audios/ folder using the Vosk offline speech recognition model.\n",
        "It first ensures the Vosk model is available (and downloads it if missing), then converts each MP3 to a 16 kHz mono WAV (the required input format). The audio is processed in chunks, and the recognized text is stored in structured JSON files under the jsons/ directory.\n",
        "Each JSON file contains the full transcript, word-level timing data, and model details â€” forming the foundation for later embedding or retrieval-based analysis."
      ],
      "metadata": {
        "id": "B8I1AfcZJgru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Cell 5: Transcribe all mp3 files in audios/ â†’ jsons/ using Vosk\n",
        "import os, json, pathlib, subprocess, shlex\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Folders\n",
        "AUDIO_DIR = \"audios\"\n",
        "JSON_DIR = \"jsons\"\n",
        "MODEL_DIR = \"vosk_model\"\n",
        "\n",
        "os.makedirs(JSON_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Attempt to download a small English model if not present\n",
        "MODEL_PATH = Path(MODEL_DIR)\n",
        "if not any(MODEL_PATH.iterdir()):\n",
        "    print(\"No Vosk model found, downloading small English model (if internet available)...\")\n",
        "    try:\n",
        "        model_url = \"https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\"\n",
        "        zip_path = \"/tmp/vosk_model.zip\"\n",
        "        subprocess.run(f\"wget -q -O {zip_path} {model_url}\", shell=True, check=True)\n",
        "        subprocess.run(f\"unzip -q {zip_path} -d {MODEL_DIR}\", shell=True, check=True)\n",
        "        # Move contents up one level if nested\n",
        "        inner = next(Path(MODEL_DIR).glob(\"vosk-model-*\"), None)\n",
        "        if inner:\n",
        "            for f in inner.iterdir():\n",
        "                subprocess.run(f\"mv {f} {MODEL_DIR}/\", shell=True)\n",
        "            subprocess.run(f\"rm -rf {inner}\", shell=True)\n",
        "        print(\"âœ… Model downloaded and extracted to\", MODEL_DIR)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Could not download Vosk model automatically:\", e)\n",
        "        print(\"â†’ Please manually upload a model folder into\", MODEL_DIR)\n",
        "\n",
        "# Import vosk\n",
        "from vosk import Model, KaldiRecognizer\n",
        "import wave\n",
        "\n",
        "# Load model\n",
        "print(\"Loading Vosk model from:\", MODEL_DIR)\n",
        "model = Model(MODEL_DIR)\n",
        "\n",
        "# Helper: convert mp3 â†’ 16 kHz mono WAV (Vosk requires WAV)\n",
        "def mp3_to_wav_16k(src_mp3, dst_wav):\n",
        "    cmd = f'ffmpeg -y -i {shlex.quote(src_mp3)} -ac 1 -ar 16000 -vn {shlex.quote(dst_wav)}'\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "# Process each MP3\n",
        "mp3_files = sorted(Path(AUDIO_DIR).glob(\"*.mp3\"))\n",
        "if not mp3_files:\n",
        "    print(\"âš ï¸ No mp3 files found in\", AUDIO_DIR)\n",
        "else:\n",
        "    for mp3 in tqdm(mp3_files, desc=\"Transcribing MP3s\"):\n",
        "        try:\n",
        "            wav_path = Path(\"/tmp\") / (mp3.stem + \"_16k.wav\")\n",
        "            mp3_to_wav_16k(str(mp3), str(wav_path))\n",
        "\n",
        "            wf = wave.open(str(wav_path), \"rb\")\n",
        "            rec = KaldiRecognizer(model, 16000)\n",
        "            rec.SetWords(True)\n",
        "\n",
        "            results = []\n",
        "            while True:\n",
        "                data = wf.readframes(4000)\n",
        "                if len(data) == 0:\n",
        "                    break\n",
        "                if rec.AcceptWaveform(data):\n",
        "                    part = json.loads(rec.Result())\n",
        "                    results.append(part)\n",
        "            results.append(json.loads(rec.FinalResult()))\n",
        "            wf.close()\n",
        "\n",
        "            text = \" \".join([r.get(\"text\", \"\") for r in results]).strip()\n",
        "            out = {\n",
        "                \"file\": mp3.name,\n",
        "                \"text\": text,\n",
        "                \"segments\": results,\n",
        "                \"model\": \"vosk-small-en-us-0.15\"\n",
        "            }\n",
        "\n",
        "            out_path = Path(JSON_DIR) / (mp3.stem + \".json\")\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
        "                json.dump(out, fh, ensure_ascii=False, indent=2)\n",
        "            print(\"âœ… Saved transcript:\", out_path)\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Error transcribing\", mp3, \":\", e)\n"
      ],
      "metadata": {
        "id": "I4SzxaPiAL_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ§¹Cell 6 Description:**\n",
        "\n",
        "This cell performs **basic preprocessing and cleanup** of the raw transcription JSON files generated by Vosk.\n",
        "It reads each JSON file from the `jsons/` directory, removes unnecessary metadata, and keeps only the essential fields â€” the filename, transcribed text, segments, and language (if available).\n",
        "The cleaned and lightweight versions are then saved into a new folder, `jsons/clean`, making them ready for **faster downstream processing**, such as text embedding or retrieval-based question answering.\n"
      ],
      "metadata": {
        "id": "4SaJU_3vJw9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Minimal preprocessing fallback - creates cleaned JSONs in jsons/clean_\n",
        "import os, json, glob\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT_DIR = \"jsons\"\n",
        "OUT_DIR = \"jsons/clean\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "for f in Path(INPUT_DIR).glob(\"*.json\"):\n",
        "    try:\n",
        "        j = json.load(open(f, \"r\", encoding=\"utf-8\"))\n",
        "        cleaned = {\n",
        "            \"file\": f.name,\n",
        "            \"text\": j.get(\"text\", \"\"),\n",
        "            \"segments\": j.get(\"segments\", []),\n",
        "            \"language\": j.get(\"language\", \"\")\n",
        "        }\n",
        "        out = Path(OUT_DIR) / f.name\n",
        "        json.dump(cleaned, open(out, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
        "        print(\"Cleaned ->\", out)\n",
        "    except Exception as e:\n",
        "        print(\"Skip\", f, e)\n"
      ],
      "metadata": {
        "id": "rQ6Y48GwOMky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§¬ **Cell 7 Description:**\n",
        "\n",
        "This cell builds (or updates) your **embeddings dataset** from the cleaned transcripts in `jsons/clean`. It assembles a DataFrame of text chunks, **reuses any existing embeddings** from `embeddings.joblib`, computes only the **missing vectors** using `SentenceTransformer` (`all-MiniLM-L6-v2`), saves the result back to `embeddings.joblib`, and exposes it as `embeddings_df` for downstream RAG queries.\n"
      ],
      "metadata": {
        "id": "TdjOV2-4KIDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - Ensure embeddings.joblib exists (create from jsons/clean if missing)\n",
        "# Run this cell in the project root where jsons/clean and your scripts live.\n",
        "\n",
        "# 0) Install dependencies (Colab-friendly). Comment out if already installed.\n",
        "!pip install -q sentence-transformers joblib pandas tqdm\n",
        "\n",
        "# --- Begin Python logic ---\n",
        "import os, glob, json, joblib, math\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "EMB_PATH = \"embeddings.joblib\"\n",
        "CLEAN_JSON_DIR = \"jsons/clean\"   # change if your cleaned JSONs are elsewhere\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"  # small and fast; change if desired\n",
        "BATCH_SIZE = 64                  # memory vs speed tradeoff\n",
        "\n",
        "def load_json_chunks(jpath):\n",
        "    \"\"\"Return list of dicts with at least 'text' (and optionally start/end).\"\"\"\n",
        "    with open(jpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    # Common structures: list of chunks OR dict with 'chunks' OR dict with 'segments'/'results'\n",
        "    if isinstance(data, list):\n",
        "        chunks = data\n",
        "    elif isinstance(data, dict) and \"chunks\" in data and isinstance(data[\"chunks\"], list):\n",
        "        chunks = data[\"chunks\"]\n",
        "    elif isinstance(data, dict) and (\"segments\" in data or \"results\" in data):\n",
        "        chunks = data.get(\"segments\", []) or data.get(\"results\", [])\n",
        "    else:\n",
        "        # fallback: treat top-level dict as a single chunk if it has 'text' or 'transcript'\n",
        "        if isinstance(data, dict) and (\"text\" in data or \"transcript\" in data):\n",
        "            chunks = [data]\n",
        "        else:\n",
        "            chunks = []\n",
        "    # Normalize each chunk to a dict with text/start/end\n",
        "    out = []\n",
        "    for c in chunks:\n",
        "        if not isinstance(c, dict):\n",
        "            continue\n",
        "        text = c.get(\"text\") or c.get(\"transcript\") or \"\"\n",
        "        text = text.strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        start = c.get(\"start\") or c.get(\"start_time\") or c.get(\"start_sec\") or None\n",
        "        end   = c.get(\"end\")   or c.get(\"end_time\")   or c.get(\"end_sec\")   or None\n",
        "        out.append({\"text\": text, \"start\": start, \"end\": end})\n",
        "    return out\n",
        "\n",
        "def build_dataframe_from_jsons(json_dir):\n",
        "    files = sorted(glob.glob(os.path.join(json_dir, \"*.json\")))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No JSON files found in {json_dir}. Check path.\")\n",
        "    rows = []\n",
        "    for jf in files:\n",
        "        chunks = load_json_chunks(jf)\n",
        "        if not chunks:\n",
        "            # If load_json_chunks returned nothing, attempt simple fallback\n",
        "            try:\n",
        "                with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
        "                    raw = f.read().strip()\n",
        "                    if raw:\n",
        "                        rows.append({\"file\": os.path.basename(jf), \"text\": raw, \"start\": None, \"end\": None})\n",
        "            except Exception:\n",
        "                pass\n",
        "            continue\n",
        "        for c in chunks:\n",
        "            rows.append({\n",
        "                \"file\": os.path.basename(jf),\n",
        "                \"text\": c[\"text\"],\n",
        "                \"start\": c[\"start\"],\n",
        "                \"end\": c[\"end\"]\n",
        "            })\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"No transcript text extracted from JSONs.\")\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"id\"] = df.index.astype(str)\n",
        "    return df\n",
        "\n",
        "# 1) If embeddings.joblib exists, load it and try to preserve embeddings\n",
        "if os.path.exists(EMB_PATH):\n",
        "    try:\n",
        "        print(\"Loading existing\", EMB_PATH)\n",
        "        existing = joblib.load(EMB_PATH)\n",
        "        if not isinstance(existing, pd.DataFrame):\n",
        "            print(\"Warning: existing embeddings.joblib not a DataFrame â€” will overwrite.\")\n",
        "            existing = None\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load existing embeddings.joblib:\", e)\n",
        "        existing = None\n",
        "else:\n",
        "    existing = None\n",
        "\n",
        "# 2) Build dataframe from cleaned jsons\n",
        "print(\"Building transcript DataFrame from JSONs in:\", CLEAN_JSON_DIR)\n",
        "df = build_dataframe_from_jsons(CLEAN_JSON_DIR)\n",
        "\n",
        "# 3) If existing DF present and has 'id' and 'embedding', merge to keep embeddings\n",
        "if existing is not None and \"id\" in existing.columns:\n",
        "    # Align by 'id' if possible, otherwise fallback to merging by file+text\n",
        "    if set(existing[\"id\"].astype(str)).issuperset(set(df[\"id\"].astype(str))):\n",
        "        # simple replacement: take embeddings from existing where id matches\n",
        "        existing_idxed = existing.set_index(existing[\"id\"].astype(str))\n",
        "        df = df.set_index(df[\"id\"].astype(str))\n",
        "        df[\"embedding\"] = existing_idxed[\"embedding\"]\n",
        "        df = df.reset_index(drop=True)\n",
        "        # reassign id as string index\n",
        "        df[\"id\"] = df.index.astype(str)\n",
        "        print(\"Merged existing embeddings by id.\")\n",
        "    else:\n",
        "        # fallback: merge on file+text (slower but robust)\n",
        "        merged = df.merge(existing[[\"file\",\"text\",\"embedding\"]], on=[\"file\",\"text\"], how=\"left\")\n",
        "        df = merged.rename(columns={\"embedding\": \"embedding\"}).copy()\n",
        "        df[\"id\"] = df.index.astype(str)\n",
        "        print(\"Merged existing embeddings by file+text where possible.\")\n",
        "else:\n",
        "    # ensure embedding column exists\n",
        "    df[\"embedding\"] = None\n",
        "\n",
        "# 4) Find which rows need embeddings\n",
        "missing_mask = df[\"embedding\"].isna() | df[\"embedding\"].apply(lambda x: x is None)\n",
        "to_compute_texts = df.loc[missing_mask, \"text\"].astype(str).tolist()\n",
        "print(f\"Total rows: {len(df)} | Missing embeddings: {len(to_compute_texts)}\")\n",
        "\n",
        "# 5) Compute embeddings if needed\n",
        "if len(to_compute_texts) > 0:\n",
        "    print(\"Loading SBERT model:\", MODEL_NAME)\n",
        "    model = SentenceTransformer(MODEL_NAME)\n",
        "    # We'll compute embeddings in batches to avoid memory spikes\n",
        "    n = len(to_compute_texts)\n",
        "    batches = math.ceil(n / BATCH_SIZE)\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(batches), desc=\"embedding batches\"):\n",
        "        start = i * BATCH_SIZE\n",
        "        end = min((i+1) * BATCH_SIZE, n)\n",
        "        batch_texts = to_compute_texts[start:end]\n",
        "        emb_batch = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True, batch_size=BATCH_SIZE)\n",
        "        embeddings.append(emb_batch)\n",
        "    import numpy as np\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    # Put embeddings back into df\n",
        "    idxs = df.loc[missing_mask].index.tolist()\n",
        "    if len(idxs) != embeddings.shape[0]:\n",
        "        raise RuntimeError(\"Mismatch between indices to fill and number of embeddings computed.\")\n",
        "    for i, idx in enumerate(idxs):\n",
        "        df.at[idx, \"embedding\"] = embeddings[i].tolist()\n",
        "    # Save to joblib\n",
        "    joblib.dump(df, EMB_PATH)\n",
        "    print(\"âœ… Saved embeddings.joblib with\", len(df), \"rows.\")\n",
        "else:\n",
        "    print(\"No missing embeddings. Using existing embeddings.joblib as-is.\")\n",
        "\n",
        "# 6) Quick sanity checks and expose df as embeddings_df to be used by downstream cells\n",
        "embeddings_df = df  # name downstream code can use\n",
        "print(\"embeddings_df ready. Sample columns:\", embeddings_df.columns.tolist())\n",
        "if embeddings_df.loc[embeddings_df[\"embedding\"].notna()].shape[0] > 0:\n",
        "    sample_emb = embeddings_df.loc[embeddings_df[\"embedding\"].notna(), \"embedding\"].iloc[0]\n",
        "    print(\"Sample embedding length:\", len(sample_emb))\n",
        "else:\n",
        "    print(\"Warning: no embeddings present after run.\")\n"
      ],
      "metadata": {
        "id": "5G4HdNdHQUsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¤– **Cell 8 Description:**\n",
        "\n",
        "This cell enables **Retrieval-Augmented Generation (RAG)** using either **Groq** or **Gemini** as the language model backend.\n",
        "It loads the precomputed **embeddings** into a **FAISS** index for fast similarity search, retrieves the most relevant text chunks for each query, and then feeds them into a powerful LLM (Groq or Gemini) to generate **context-aware answers**.\n",
        "If neither API key is set, it prompts you to configure one. Once active, you can **interactively ask questions**, and the assistant will respond using only the provided course or lecture content â€” ideal for creating an **AI-powered teaching assistant** experience.\n"
      ],
      "metadata": {
        "id": "kUSIXYCbKn9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# âœ… CELL 8 â€” RAG with Groq or Gemini (auto-pick model)\n",
        "# ===========================\n",
        "!pip -q install faiss-cpu sentence-transformers joblib groq google-generativeai\n",
        "\n",
        "import os, numpy as np, joblib, faiss, sys, traceback\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('RAG-API')\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load embeddings + index\n",
        "# -----------------------------\n",
        "embeddings_df = joblib.load(\"embeddings.joblib\")\n",
        "emb_matrix = np.vstack(embeddings_df[\"embedding\"].values).astype(\"float32\")\n",
        "texts = embeddings_df[\"text\"].tolist()\n",
        "\n",
        "index = faiss.IndexFlatL2(emb_matrix.shape[1])\n",
        "index.add(emb_matrix)\n",
        "print(f\"âœ… Loaded {len(texts)} chunks into FAISS.\")\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def retrieve_context(query, top_k=4):\n",
        "    q = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "    _, idx = index.search(q, top_k)\n",
        "    return \"\\n\\n\".join([f\"[{r+1}] {texts[i]}\" for r, i in enumerate(idx[0])])\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Pick provider (GROQ/GEMINI)\n",
        "# -----------------------------\n",
        "PROVIDER = \"GROQ\" if os.getenv(\"GROQ_API_KEY\") else (\"GEMINI\" if os.getenv(\"GEMINI_API_KEY\") else None)\n",
        "if not PROVIDER:\n",
        "    raise RuntimeError(\n",
        "        \"No provider configured. Set one of:\\n\"\n",
        "        \"  os.environ['GROQ_API_KEY']   = '...'\\n\"\n",
        "        \"  os.environ['GEMINI_API_KEY'] = '...'\\n\"\n",
        "        \"Then re-run this cell.\"\n",
        "    )\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a precise teaching assistant. Use ONLY the provided context. \"\n",
        "    \"If the answer isn't in the context, say you don't know briefly.\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Provider-specific setup\n",
        "# -----------------------------\n",
        "if PROVIDER == \"GROQ\":\n",
        "    from groq import Groq\n",
        "    groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "    # List models available to this key and auto-pick a good one\n",
        "    try:\n",
        "        available_models = sorted([m.id for m in groq_client.models.list().data])\n",
        "    except Exception:\n",
        "        print(\"âš ï¸ Could not list Groq models. Trying known names.\")\n",
        "        available_models = []\n",
        "\n",
        "    print(\"ðŸ”Ž Groq models available:\\n\", \"\\n \".join(available_models) or \"(listing failed)\")\n",
        "\n",
        "    PREFERRED = [\n",
        "        # common currently-supported ids (adjusts over time)\n",
        "        \"llama-3.1-70b-specdec\",\n",
        "        \"llama-3.1-8b-instant\",\n",
        "        \"mixtral-8x7b-32768\",\n",
        "        \"gemma2-9b-it\",\n",
        "        # fallbacks some accounts expose\n",
        "        \"llama3-70b-8192\",\n",
        "        \"llama3-8b-8192\",\n",
        "    ]\n",
        "    MODEL = next((m for m in PREFERRED if (not available_models or m in available_models)), None)\n",
        "    if not MODEL:\n",
        "        raise RuntimeError(\n",
        "            \"No preferred Groq models found for your key. Enable one in Groq console, \"\n",
        "            \"or replace PREFERRED with a model id you do have.\"\n",
        "        )\n",
        "\n",
        "    print(f\"âœ… Using Groq model: {MODEL}\")\n",
        "\n",
        "    def generate_answer(query, context):\n",
        "        prompt = f\"{SYSTEM_PROMPT}\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer:\"\n",
        "        try:\n",
        "            resp = groq_client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.2,\n",
        "                max_tokens=512,\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"[Generation error: {e}]\"\n",
        "\n",
        "elif PROVIDER == \"GEMINI\":\n",
        "    import google.generativeai as genai\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "    def generate_answer(query, context):\n",
        "        prompt = f\"{SYSTEM_PROMPT}\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer:\"\n",
        "        try:\n",
        "            resp = model.generate_content(prompt)\n",
        "            return (resp.text or \"\").strip()\n",
        "        except Exception as e:\n",
        "            return f\"[Generation error: {e}]\"\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Interactive QA\n",
        "# -----------------------------\n",
        "def ask_question():\n",
        "    print(f\"ðŸ”Œ Provider: {PROVIDER}\\nType 'exit' to quit.\")\n",
        "    while True:\n",
        "        q = input(\"\\nâ“ Your question: \").strip()\n",
        "        if q.lower() in (\"exit\",\"quit\"):\n",
        "            print(\"ðŸ‘‹ Bye!\"); break\n",
        "        ctx = retrieve_context(q, top_k=4)\n",
        "        print(\"\\nðŸ”Ž Retrieved Context:\\n\", (ctx[:800] + (\"...\" if len(ctx) > 800 else \"\")))\n",
        "        ans = generate_answer(q, ctx)\n",
        "        print(\"\\nðŸ’¬ Answer:\\n\", ans)\n",
        "\n",
        "ask_question()\n"
      ],
      "metadata": {
        "id": "wV1z58_8AS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© Project Summary**\n",
        "\n",
        "**Project Title: ðŸ§  RAG-Based Teaching AI Assistant (Offline Vosk + Embeddings + RAG Integration)**\n",
        "\n",
        "This project builds a Retrieval-Augmented Generation (RAG) pipeline that converts video lectures into structured, searchable text and enables an AI-powered question-answering system â€” all while maintaining offline speech recognition capabilities using Vosk.\n",
        "\n",
        "The system automatically:\n",
        "\n",
        "Imports your teaching project folder directly from Google Drive.\n",
        "\n",
        "Converts all uploaded video lectures (.mp4) into audio files (.mp3) using ffmpeg.\n",
        "\n",
        "Transcribes those audios offline using the Vosk speech recognition model.\n",
        "\n",
        "Cleans and structures the resulting text data into lightweight JSON files.\n",
        "\n",
        "Creates semantic embeddings using SentenceTransformers for context retrieval.\n",
        "\n",
        "Builds a FAISS vector index for efficient similarity search.\n",
        "\n",
        "Integrates with Groq or Gemini LLMs for natural, context-aware question answering.\n",
        "\n",
        "*Ultimately, this notebook allows educators, students, and developers to query lecture content interactively â€” creating a personalized AI teaching assistant trained on their own material.*"
      ],
      "metadata": {
        "id": "IzCGZ1p7LzNE"
      }
    }
  ]
}